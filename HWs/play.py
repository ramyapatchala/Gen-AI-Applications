import time
import os
import logging
from dotenv import load_dotenv
load_dotenv("test.env")
logging.basicConfig(filename = 'test_models_debug.txt', level=logging.INFO)
question_to_ask = "Summarize this following document in 100 words. Veuillez résumer le document en français. \n\n\n\n\n This document demonstrates that any neural network can be represented as an exact decision tree, rather than an approximation, without altering the architecture of the neural network. This equivalence enables a better understanding of the network and addresses its black-box nature. The decision tree can be constructed for various neural network components, such as fully connected and convolutional layers, skip connections, normalizations, and recurrent networks, ensuring a comprehensive representation. Experimental results on toy problems validate the approach and showcase its interpretability and computational advantages. This method promises to enhance the understanding and application of neural networks in various industries.', generation_id='69e3890d-7c96-4c58-8e75-4276b80c7bb8', citations=None, documents=None, is_search_required=None, search_queries=None, search_results=None, finish_reason='COMPLETE', tool_calls=None, chat_history=[Message_User(role='USER', message='Document: Neural Networks are Decision Trees\nCaglar Aytekin\nAI Lead\nAAC Technologies\ncaglaraytekin@aactechnologies.com, cagosmail@gmail.com\nAbstract\nIn this manuscript, we show that any neural network\nwith any activation function can be represented as a de-\ncision tree. The representation is equivalence and not an\napproximation, thus keeping the accuracy of the neural net-\nwork exactly as is. We believe that this work provides bet-\nter understanding of neural networks and paves the way to\ntackle their black-box nature. We share equivalent trees\nof some neural networks and show that besides providing\ninterpretability, tree representation can also achieve some\ncomputational advantages for small networks. The analysis\nholds both for fully connected and convolutional networks,\nwhich may or may not also include skip connections and/or\nnormalizations.\n1. Introduction\nDespite the immense success of neural networks over the\npast decade, the black-box nature of their predictions pre-\nvent their wider and more reliable adoption in many indus-\ntries, such as health and security. This fact led researchers to\ninvestigate ways to explain neural network decisions. The\nefforts in explaining neural network decisions can be cate-\ngorized into several approaches: saliency maps, approxima-\ntion by interpretable methods and joint models.\nSaliency maps are ways of highlighting areas on the in-\nput, of which a neural network make use of the most while\nprediction. An earlier work [20] takes the gradient of the\nneural network output with respect to the input in order to\nvisualize an input-speciﬁc linearization of the entire net-\nwork. Another work [26] uses a deconvnet to go back to\nfeatures from decisions. The saliency maps obtained via\nthese methods are often noisy and prevent a clear under-\nstanding of the decisions made. Another track of meth-\nods [29], [18], [4], [6] make use of the derivative of a neural\nnetwork output with respect to an activation, usually the one\nright before fully connected layers. This saliency maps ob-\ntained by this track, and some other works [27], [11], [5] are\nclearer in the sense of highlighting areas related to the pre-\ndicted class. Although useful for purposes such as check-\ning whether the support area for decisions are sound, these\nmethods still lack a detailed logical reasoning of why such\ndecision is made.\nConversion between neural networks and interpretable\nby-design models -such as decision trees- has been a topic\nof interest. In [8], a method was devised to initialize neural\nnetworks with decision trees. [9, 19, 25] also provides neu-\nral network equivalents of decision trees. The neural net-\nworks in these works have speciﬁc architectures, thus the\nconversion lacks generalization to any model. In [24], neu-\nral networks were trained in such a way that their decision\nboundaries can be approximated by trees. This work does\nnot provide a correspondence between neural networks and\ndecision trees, and merely uses the latter as a regulariza-\ntion. In [7], a neural network was used to train a decision\ntree. Such tree distillation is an approximation of a neural\nnetwork and not a direct conversion, thus performs poorly\non the tasks that the neural network was trained on.\nJoint neural network and decision tree models [12], [16],\n[13], [14], [17], [2], [10], [22] genarally use deep learning to\nassists some trees, or come up with a neural network struc-\nture so it resembles a tree. A recent work [23] replaces the\nﬁnal fully connected layer of a neural network with a deci-\nsion tree. Since the backbone features are still that of neural\nnetworks, the explanation is sought to be achieved via pro-\nviding a means to humans to validate the decision as a good\nor bad one, rather than a complete logical reasoning of the\ndecision.\nIn this paper, we show that any neural network having\nany activations has a directly equivalent decision tree rep-\nresentation. Thus, the induced tree output is exactly the\nsame with that of the neural network and tree representation\ndoesn’t limit or require altering of the neural architecture\nin any way. We believe that the decision tree equivalence\nprovides better understanding of neural networks and paves\nthe way to tackle the black-box nature of neural networks,\ne.g. via analyzing the category that a test sample belongs\nto, which can be extracted by the node rules that a sample\nis categorized. We show that the decision tree equivalent of\narXiv:2210.05189v3  [cs.LG]  25 Oct 2022\na neural network can be found for either fully connected or\nconvolutional neural networks which may include skip lay-\ners and normalizations as well. Besides the interpretability\naspect, we show that the induced tree is also advantageous\nto the corresponding neural network in terms of computa-\ntional complexity, at the expense of increased storage mem-\nory.\nUpon writing this paper, we have noticed the following\nworks having overlaps with ours [28], [3], [15], [21], es-\npecially for feedforward ReLU networks. We extend the\nﬁndings in these works to any activation function and also\nrecurrent neural networks.\n2. Decision Tree Analysis of Neural Networks\nThe derivations in this section will be ﬁrst made for feed-\nforward neural networks with piece-wise linear activation\nfunctions such as ReLU, Leaky ReLU, etc. Next, the analy-\nsis will be extended to any neural network with any activa-\ntion function.\n2.1. Fully Connected Networks\nLet Wi be the weight matrix of a network’s ith layer.\nLet σ be any piece-wise linear activation function, and x0\nbe the input to the neural network. Then, the output and an\nintermediate feature of a feed-forward neural network can\nbe represented as in Eq. 1.\nNN(x0) = WT\nn−1σ(WT\nn−2σ(...WT\n1 σ(WT\n0 x0)))\nxi = σ(WT\ni−1σ(...WT\n1 σ(WT\n0 x0)))\n(1)\nNote that in Eq. 1, we omit any ﬁnal activation (e.g.\nsoftmax) and we ignore the bias term as it can be simply\nincluded by concatenating a 1 value to each xi. The acti-\nvation function σ acts as an element-wise scalar multiplica-\ntion, hence the following can be written.\nWT\ni σ(WT\ni−1xi−1) = WT\ni (ai−1 ⊙(WT\ni−1xi−1))\n(2)\nIn Eq. 2, ai−1 is a vector indicating the slopes of activa-\ntions in the corresponding linear regions where WT\ni−1xi−1\nfall into, ⊙denotes element-wise multiplication. Note that,\nai−1 can directly be interpreted as a categorization result\nsince it includes indicators (slopes) of linear regions in ac-\ntivation function. The Eq. 2 can be re-organized as follows.\nWT\ni σ(WT\ni−1xi−1) = (Wi ⊙ai−1)T WT\ni−1xi−1\n(3)\nIn Eq.\n3, we use ⊙as a column-wise element-wise\nmultiplication on Wi. This corresponds to element-wise\nmultiplication by a matrix obtained via by repeating ai−1\ncolumn-vector to match the size of Wi. Using Eq. 3, Eq. 1\ncan be rewritten as follows.\nNN(x0) = (Wn−1 ⊙an−2)T (Wn−2 ⊙an−3)T\n...(W1 ⊙a0)T WT\n0 x0\n(4)\nFrom Eq. 4, one can deﬁne an effective weight matrix\nˆW\nT\ni of a layer i to be applied directly on input x0 as follows:\nCi−1 ˆW\nT\ni = (Wi ⊙ai−1)T ...(W1 ⊙a0)T WT\n0\nCi−1 ˆW\nT\ni x0 = WT\ni xi\n(5)\nIn Eq. 5, the categorization vector until layer i is de-\nﬁned as follows: ci−1 = a0 ∥a1 ∥...ai−1, where ∥is the\nconcatenation operator.\nOne can directly observe from Eq.\n5 that, the effec-\ntive matrix of layer i is only dependent on the categoriza-\ntion vectors from previous layers. This indicates that in\neach layer, a new efﬁcient ﬁlter is selected -to be applied\non the network input- based on the previous categoriza-\ntions/decisions. This directly shows that a fully connected\nneural network can be represented as a single decision tree,\nwhere effective matrices acts as categorization rules.\nIn\neach layer i, response of effective matrix Ci−1 ˆW\nT\ni is catego-\nrized into ai vector, and based on this categorization result,\nnext layer’s effective matrix Ci ˆW\nT\ni+1 is determined. A layer\ni is thus represented as kmi-way categorization, where mi is\nthe number ﬁlters in each layer i and k is the total number\nof linear regions in an activation. This categorization in a\nlayer i can thus be represented by a tree of depth mi, where\na node in any depth is branched into k categorizations.\nIn order to better illustrate the equivalent decision tree of\na neural network, in Algorithm 1, we rewrite Eq. 5 for the\nentire network, as an algorithm. For the sake of simplic-\nity and without loss of generality, we provide the algorithm\nwith the ReLU activation function, where a ∈{0, 1}. It\ncan be clearly observed that, the lines 5 −9 in Algorithm 1\ncorresponds to a node in the decision tree, where a simple\nyes/no decision is made.\nThe decision tree equivalent of a neural network can thus\nbe constructed as in Algorithm 2. Using this algorithm,\nwe share a a tree representation obtained for a neural net-\nwork with three layers, having 2,1 and 1 ﬁlter for layer 1,\n2 and 3 respectively. The network has ReLU activation in\nbetween layers, and no activation after last layer. It can be\nobserved from Algorithm 2 and Fig. 1 that the depth of a\nNN-equivalent tree is d = Pn−2\ni=0 mi, and total number of\ncategories in last branch is 2d. At ﬁrst glance, the number\nof categories seem huge. For example, if ﬁrst layer of a neu-\nral network contains 64 ﬁlters, there would exist at least 264\nbranches in a tree, which is already intractable. But, there\nWT\n00x0 > 0\nWT\n01x0 > 0\n00 ˆ\nW10\nT x0 > 0\n000 ˆ\nW20\nT x0\n001 ˆ\nW20\nT x0\n01 ˆ\nW10\nT x0 > 0\n010 ˆ\nW20\nT x0\n011 ˆ\nW20\nT x0\nWT\n01x0 > 0\n10 ˆ\nW10\nT x0 > 0\n100 ˆ\nW20\nT x0\n101 ˆ\nW20\nT x0\n11 ˆ\nW10\nT x0 > 0\n110 ˆ\nW20\nT x0\n111 ˆ\nW20\nT x0\nFigure 1. Decision Tree for a 2-layer ReLU Neural Network\nmay occur violating and redundant rules that would pro-\nvide lossless pruning of the NN-equivalent tree. Another\nobservation is that, it is highly likely that not all categories\nwill be realized during training due to the possibly much\nlarger number of categories (tree leaves) than training data.\nThese categories can be pruned as well based on the ap-\nplication, and the data falling into these categories can be\nconsidered invalid, if the application permits. In the next\nsection, we show that such redundant, violating and unre-\nalized categories indeed exist, by analysing decision trees\nof some neural networks. But before that, we show that the\ntree equivalent of a neural network exists for skip connec-\ntions, normalizations, convolutions, other activation func-\ntions and recurrence.\n2.1.1\nSkip Connections\nWe analyse a residual neural network of the following type:\nrx0 = WT\n0 x0\nrxi = rxi−1 + WT\ni σ(rxi−1)\n(6)\nUsing Eq. 6, via a similar analysis in Sec. 2.1, one can\nrewrite rxi as follows.\nrxi = ai−1 ˆW\nT\ni rxi−1\nai−1 ˆW\nT\ni = I + (Wi ⊙ai−1)T\n(7)\nFinally, using ai−1 ˆW\nT\ni in Eq. 7, one can deﬁne effective\nmatrices for residual neural networks as follows.\nrxi = r ˆW\nT\ni x0\nr ˆW\nT\ni = ai−1 ˆW\nT\ni ai−2 ˆW\nT\ni−1...a0 ˆW\nT\n1 WT\n0\n(8)\nOne can observe from Eq. 8 that, for layer i, the residual\neffective matrix r ˆW\nT\ni is deﬁned solely based on categoriza-\ntions from the previous activations. Similar to the analysis\nAlgorithm 1: Algorithm of Eq. 5 for ReLU net-\nworks\n1 ˆW = W0\n2 for i = 0 to n −2 do\n3\na = []\n4\nfor j = 0 to mi −1 do\n5\nif ˆW\nT\nijx0 > 0 then\n6\na.append(1)\n7\nelse\n8\na.append(0)\n9\nend\n10\nend\n11\nˆW = ˆW(Wi+1 ⊙a)\n12 end\n13 return ˆW\nT x0\nAlgorithm 2: Algorithm of converting neural net-\nworks to decision trees\n1 Initialize Tree: Set root.\n2 Branch all leafs to k nodes, decision rule is ﬁrst\neffective ﬁlter.\n3 Branch all nodes to k more nodes, and repeat until\nall effective ﬁlters in a layer is covered.\n4 Calculate effective matrix for each leaf via Eq. 5.\nRepeat 2,3.\n5 Repeat 4 until all layers are covered.\n6 return Tree\nin Sec. 2.1, this enables a tree equivalent of residual neural\nnetworks.\n2.1.2\nNormalization Layers\nA separate analysis is not needed for any normalization\nlayer, as popular normalization layers are linear, and after\ntraining, they can be embedded into the linear layer that it\ncomes after or before, in pre-activation or post-activation\nnormalizations respectively.\n2.2. Convolutional Neural Networks\nLet Ki : Ci+1 ×Ci ×Mi ×Ni be the convolution kernel\nfor layer i, applying on an input Fi : Ci × Hi × Wi. Note\nthat Mi and Ni denote the spatial size of the convolutional\nkernel, and Hi and Wi denote the spatial size of the input.\nOne can write the output of a convolutional neural net-\nwork CNN(F0), and an intermediate feature Fi as follows.\nCNN(F0) = Kn−1 ∗σ(Kn−2 ∗σ(...σ(K0 ∗F0))\nFi = σ(Ki−1 ∗σ(...σ(K0 ∗F0))\n(9)\nSimilar to the fully connected network analysis, one can\nwrite the following, due to element-wise scalar multiplica-\ntion nature of the activation function.\nKi ∗σ(Ki−1 ∗Fi−1) = (Ki ⊙ai−1) ∗(Ki−1 ∗Fi−1) (10)\nIn Eq. 10, ai−1 is of same spatial size as Ki and consists\nof the slopes of activation function in corresponding regions\nin the previous feature Fi−1. Note that the above only holds\nfor a speciﬁc spatial region, and there exists a separate ai−1\nfor each spatial region that the convolution Ki−1 is applied\nto. For example, if Ki−1 is a 3 × 3 kernel, there exists a\nseparate ai−1 for all 3 × 3 regions that the convolution is\napplied to. An effective convolution Ci−1 ˆKi can be written\nas follows.\nci−1 ˆKi = (Ki ⊙ai−1) ∗... ∗(K1 ⊙a0) ∗K0\nci−1 ˆKi ∗x0 = Ki ∗xi\n(11)\nNote that in Eq. 11, Ci−1 ˆKi contains speciﬁc effective\nconvolutions per region, where a region is deﬁned accord-\ning to the receptive ﬁeld of layer i. c is deﬁned as the con-\ncatenated categorization results of all relevant regions from\nprevious layers.\nOne can observe from Eq.\n11 that effecive convolu-\ntions are only dependent on categorizations coming from\nactivations, which enables the tree equivalence -similar to\nthe analysis for fully connected network. A difference from\nfully connected layer case is that many decisions are made\non partial input regions rather than entire x0.\n2.3. Continuous Activation Functions\nIn Eq. 2, for piece-wise linear activations, elements of a\ncan have a number of values limited by the piece-wise lin-\near regions in the activation function. This number deﬁnes\nthe number of child nodes per effective ﬁlter. The exten-\nsion to continuous activation functions is trivial as they can\nbe considered as piece-wise linear functions with inﬁnite\nregions. Therefore, for continuous activations, the neural\nnetwork equivalent tree immediately becomes inﬁnite width\neven for a single ﬁlter. This might not be a useful result, but\nwe provide this discussion here for completeness. In order\nto guarantee ﬁnite trees, one may consider using quantized\nversions of continuous activations which may result in a few\npiece-wise linear regions, hence few child nodes per activa-\ntion.\n2.4. Recurrent Networks\nAs recurrent neural networks (RNNs) can be unrolled to\nfeed-forward representation, RNNs can also be equivalently\nrepresented as decision trees. We study following recurrent\nneural network. Note that we simply omit the bias terms as\nthey can be represented by concatenating a 1 value to input\nvectors.\nh(t) = σ(WT h(t−1) + UT x(t))\no(t) = VT h(t)\n(12)\nSimilar to previous analysis, one can rewrite h(t) as fol-\nlows.\nh(t) = a(t) ⊙(WT h(t−1) + UT x(t))\n(13)\nEq. 13 can be rewritten follows.\nh(t) = a(t) ⊙(\n1\nY\nj=(t−1)\n(WT ⊙a(j)))WT h(0)\n+a(t) ⊙\nt\nX\ni=1\n(\niY\nj=(t−1)\n(WT ⊙a(j)))UT x(i)\n(14)\nNote that in Eq. 14, the product operator stands for ma-\ntrix multiplication, its steps are −1 and we consider the out-\nput of product operator to be 1 when i = t. One can rewrite\nEq. 14 by introducing cj ˆWj as follows.\nh(t) = a(t) ⊙c1 ˆW1WT h(0) + a(t) ⊙\nt\nX\ni=1\nci ˆWiUT x(i)\nci ˆW\nT\ni =\niY\nj=(t−1)\n(WT ⊙a(j))\n(15)\nx < −1.16\nx < 0.32\nx > 1\nx < −0.1\n0 ˆW\nT x0\n1 ˆW\nT x0\nx0 < −0.1\n2 ˆW\nT x\n3 ˆW\nT x\nx > 0.54\nx < 0.11\n4 ˆW\nT x\n5 ˆW\nT x\nx < 0.11\n6 ˆW\nT x\n7 ˆW\nT x\nx < 0.32\nx > 0.52\nx < −0.7\n8 ˆW\nT x\n9 ˆW\nT x\nx < −0.7\n10 ˆW\nT x\n11 ˆW\nT x\nx > 0.39\nx < −0.38\n12 ˆW\nT x\n13 ˆW\nT x\nx < −0.38\n14 ˆW\nT x\n15 ˆW\nT x\nFigure 2. Decision Tree for a y = x2 Regression Neural Network\nx < −1.16\nx < 0.32\nx > 1\n0.55x + 0.09\n3.47x −2.83\nx < 0.11.\n2.37x −0.50\n−1.00x −0.12\n−3.67x −3.22\n(a) Cleaned Tree\n(b) Neural Network Approximation of y = x2\nFigure 3. Cleaned Decision Tree for a y = x2 Regression Neural Network\nCombining Eq. 15 and Eq. 12, one can write o(t) as\nfollows.\no(t) = a(t) ˆV\nT\nc1 ˆW1WT h(0)+a(t) ˆV\nT\nt\nX\ni=1\nci ˆWiUT x(i) (16)\nEq. 16 can be further simpliﬁed to the following.\no(t) = c1 ˆZ\nT\n1 WT h(0) +\nt\nX\ni=1\nci ˆZiUT x(i)\n(17)\nIn Eq. 17, ci ˆZ\nT\ni = a(t) ˆV\nT\nci ˆWi .As one can observe from\nEq. 17, the RNN output only depends on the categoriza-\ntion vector ci, which enables the tree equivalence -similar\nto previous analysis.\nNote that for RNNs, a popular choice for σ in Eq. 12\nis tanh. As mentioned in Section 2.3, in order to provide\nﬁnite trees, one might consider using a piece-wise linear\napproximation of tanh.\n3. Experimental Results\nFirst, we make a toy experiment where we ﬁt a neural\nnetwork to: y = x2 equation. The neural network has 3\ndense layers with 2 ﬁlters each, except for last layer which\nhas 1 ﬁlter. The network uses leaky-ReLU activations after\nfully connected layers, except for the last layer which has\nno post-activation. We have used negative slope of 0.3 for\nleaky-ReLU which is the default value in Tensorﬂow [1].\nThe network was trained with 5000 (x, y) pairs where x was\nregularly sampled from [−2.5, 2.5] interval. Fig. 2 shows\nthe decision tree corresponding to the neural network. In the\ntree, every black rectangle box indicates a rule, left child\nfrom the box means the rule does not hold, and the right\nchild means the rule holds. For better visualization, the\nrules are obtained via converting wT x + β > 0 to direct\ninequalities acting on x. This can be done for the partic-\nular regression y = x2, since x is a scalar. In every leaf,\nthe network applies a linear function -indicated by a red\nrectangle- based on the decisions so far. We have avoided\nwriting these functions explicitly due to limited space. At\nﬁrst glance, the tree representation of a neural network in\nthis example seems large due to the 2\nPn−2\ni\nmi = 24 = 16\ncategorizations. However, we notice that a lot of the rules\nin the decision tree is redundant, and hence some paths in\nthe decision tree becomes invalid. An example to redundant\nrule is checking x < 0.32 after x < −1.16 rule holds. This\n−0.98x −0.49y + 0.95\n−1.6x + 0.2y −0.07\n−0.15x + 0.19y + 0.41\n1\n0.45x −0.3y + 0.05\n0\n1.6x −1.35y −1.44\n0\n1\n0\n−1.6x + 0.2y −0.07\n0.5x + 0.52y −0.22\n1\n−0.46x −0.76y + 0.94\n0\n−2.72x −3.53y + 2.77\n0\n1\n−0.5x + 0.64y −0.27\n1.51x −1y + 1.03\n1.59x −1.35y + 0.78\n0\n1\n4.18x −3.17y + 2.54\n0\n1\n1.51x −1y + 1.03\n0\n5.31x −4.51y + 3.14\n0\n1\nFigure 4. Classiﬁcation Tree for a Half-Moon Classiﬁcation Neural Network\nFigure 5. Categorizations made by the decision tree for half-moon\ndataset\ndirectly creates the invalid left child for this node. Hence,\nthe tree can be cleaned via removing the left child in this\ncase, and merging the categorization rule to the stricter one :\nx < −1.16 in the particular case. Via cleaning the decision\ntree in Fig. 2, we obtain the simpler tree in Fig. 3a, which\nonly consists of 5 categories instead of 16. The 5 categories\nare directly visible also from the model response in Fig. 3b.\nThe interpretation of the neural network is thus straightfor-\nward: for each region whose boundaries are determined via\nthe decision tree representation, the network approximates\nthe non-linear y = x2 equation by a linear equation. One\ncan clearly interpret and moreover make deduction from the\ndecision tree, some of which are as follows. The neural\nnetwork is unable to grasp the symmetrical nature of the\nregression problem which is evident from the fact that the\ndecision boundaries are asymmetrical. The region in below\n−1.16 and above 1 is unbounded and thus neural decisions\nlose accuracy as x goes beyond these boundaries.\nNext, we investigate another toy problem of classifying\nhalf-moons and analyse the decision tree produced by a neu-\nral network. We train a fully connected neural network with\ny = x2\nHalf-Moon\nParam.\nComp.\nMult./Add.\nParam.\nComp.\nMult./Add.\nTree\n14\n2.6\n2\n39\n4.1\n8.2\nNN\n13\n4\n16\n15\n5\n25\nTable 1. Computation and memory analysis of toy problems.\n3 layers with leaky-ReLU activations, except for last layer\nwhich has sigmoid activation. Each layer has 2 ﬁlters ex-\ncept for the last layer which has 1. The cleaned decision\ntree induced by the trained network is shown in Fig. 4. The\ndecision tree ﬁnds many categories whose boundaries are\ndetermined by the rules in the tree, where each category\nis assigned a single class. In order to better visualize the\ncategories, we illustrate them with different colors in Fig.\n5. One can make several deductions from the decision tree\nsuch as some regions are very well-deﬁned, bounded and\nthe classiﬁcations they make are perfectly in line with the\ntraining data, thus these regions are very reliable. There are\nunbounded categories which help obtaining accurate classi-\nﬁcation boundaries, yet fail to provide a compact represen-\ntation of the training data, these may correspond to inaccu-\nrate extrapolations made by neural decisions. There are also\nsome categories that emerged although none of the training\ndata falls to them.\nBesides the interpretability aspect, the decision tree rep-\nresentation also provides some computational advantages.\nIn Table 1, we compare the number of parameters, ﬂoat-\npoint comparisons and multiplication or addition operations\nof the neural network and the tree induced by it. Note that\nthe comparisons, multiplications and additions in the tree\nrepresentation are given as expected values, since per each\ncategory depth of the tree is different. As the induced tree\nis an unfolding of the neural network, it covers all possi-\nble routes and keeps all possible effective ﬁlters in mem-\nory. Thus, as expected, the number of parameters in the tree\nrepresentation of a neural network is larger than that of the\nnetwork. In the induced tree, in every layer i, a maximum\nof mi ﬁlters are applied directly on the input, whereas in the\nneural network always mi ﬁlters are applied on the previous\nfeature, which is usually much larger than the input in the\nfeature dimension. Thus, computation-wise, the tree repre-\nsentation is advantageous compared to the neural network\none.\n4. Conclusion\nIn this manuscript, we have shown that neural networks\ncan be equivalently represented as decision trees. The tree\nequivalence holds for fully connected layers, convolutional\nlayers, residual connections, normalizations, recurrent lay-\ners and any activation. We believe that this tree equivalence\nprovides directions to tackle the black-box nature of neural\nnetworks.\nReferences\n[1] Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy\nDavis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian\nGoodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,\nYangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\nKudlur, Josh Levenberg, Dandelion Man´e, Rajat Monga,\nSherry Moore, Derek Murray, Chris Olah, Mike Schuster,\nJonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-\nwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-\nnanda Vi´egas, Oriol Vinyals, Pete Warden, Martin Watten-\nberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor-\nFlow: Large-scale machine learning on heterogeneous sys-\ntems, 2015. Software available from tensorﬂow.org. 5\n[2] Karim Ahmed, Mohammad Haris Baig, and Lorenzo Torre-\nsani. Network of experts for large-scale image categoriza-\ntion. In European Conference on Computer Vision, pages\n516–532. Springer, 2016. 1\n[3] Randall Balestriero and Richard Baraniuk.\nMad max:\nAfﬁne spline insights into deep learning.\narXiv preprint\narXiv:1805.06576, 2018. 2\n[4] Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader,\nand Vineeth N Balasubramanian.\nGrad-cam++: General-\nized gradient-based visual explanations for deep convolu-\ntional networks. In 2018 IEEE winter conference on appli-\ncations of computer vision (WACV), pages 839–847. IEEE,\n2018. 1\n[5] Edo Collins, Radhakrishna Achanta, and Sabine Susstrunk.\nDeep feature factorization for concept discovery.\nIn Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 336–352, 2018. 1\n[6] Rachel Lea Draelos and Lawrence Carin. Use hirescam in-\nstead of grad-cam for faithful explanations of convolutional\nneural networks. arXiv e-prints, pages arXiv–2011, 2020. 1\n[7] Nicholas Frosst and Geoffrey Hinton.\nDistilling a neu-\nral network into a soft decision tree.\narXiv preprint\narXiv:1711.09784, 2017. 1\n[8] Kelli D Humbird, J Luc Peterson, and Ryan G McClar-\nren. Deep neural network initialization with decision trees.\nIEEE transactions on neural networks and learning systems,\n30(5):1286–1295, 2018. 1\n[9] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi,\nand Samuel Rota Bulo. Deep neural decision forests. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 1467–1475, 2015. 1\n[10] Mason McGill and Pietro Perona. Deciding how to decide:\nDynamic routing in artiﬁcial neural networks. In Interna-\ntional Conference on Machine Learning, pages 2363–2372.\nPMLR, 2017. 1\n[11] Mohammed Bany Muhammad and Mohammed Yeasin.\nEigen-cam: Class activation map using principal compo-\nnents.\nIn 2020 International Joint Conference on Neural\nNetworks (IJCNN), pages 1–7. IEEE, 2020. 1\n[12] Ravi Teja Mullapudi, William R Mark, Noam Shazeer, and\nKayvon Fatahalian. Hydranets: Specialized dynamic archi-\ntectures for efﬁcient inference. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 8080–8089, 2018. 1\n[13] Calvin Murdock, Zhen Li, Howard Zhou, and Tom Duerig.\nBlockout: Dynamic model selection for hierarchical deep\nnetworks. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 2583–2591,\n2016. 1\n[14] Venkatesh N Murthy, Vivek Singh, Terrence Chen, R Man-\nmatha, and Dorin Comaniciu. Deep decision network for\nmulti-class image classiﬁcation.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2240–2248, 2016. 1\n[15] Duy T Nguyen, Kathryn E Kasmarik, and Hussein A Ab-\nbass. Towards interpretable anns: An exact transformation\nto multi-class multivariate decision trees.\narXiv preprint\narXiv:2003.04675, 2020. 2\n[16] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster,\nstronger. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7263–7271, 2017. 1\n[17] Anirban Roy and Sinisa Todorovic. Monocular depth esti-\nmation using neural regression forest. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 5506–5514, 2016. 1\n[18] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam:\nVisual explanations from deep networks via\ngradient-based localization. In Proceedings of the IEEE in-\nternational conference on computer vision, pages 618–626,\n2017. 1\n[19] Ishwar Krishnan Sethi. Entropy nets: from decision trees\nto neural networks. Proceedings of the IEEE, 78(10):1605–\n1613, 1990. 1\n[20] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.\nDeep inside convolutional networks:\nVisualising image\nclassiﬁcation models and saliency maps.\narXiv preprint\narXiv:1312.6034, 2013. 1\n[21] Agus Sudjianto, William Knauth, Rahul Singh, Zebin Yang,\nand Aijun Zhang. Unwrapping the black box of deep relu\nnetworks: interpretability, diagnostics, and simpliﬁcation.\narXiv preprint arXiv:2011.04041, 2020. 2\n[22] Andreas Veit and Serge Belongie. Convolutional networks\nwith adaptive inference graphs. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), pages 3–18,\n2018. 1\n[23] Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee,\nHenry Jin, Suzanne Petryk, Sarah Adel Bargal, and Joseph E\nGonzalez.\nNbdt:\nneural-backed decision trees.\narXiv\npreprint arXiv:2004.00221, 2020. 1\n[24] Mike Wu, Michael Hughes, Sonali Parbhoo, Maurizio Zazzi,\nVolker Roth, and Finale Doshi-Velez. Beyond sparsity: Tree\nregularization of deep models for interpretability. In Pro-\nceedings of the AAAI conference on artiﬁcial intelligence,\nvolume 32, 2018. 1\n[25] Yongxin Yang, Irene Garcia Morillo, and Timothy M\nHospedales.\nDeep neural decision trees.\narXiv preprint\narXiv:1806.06988, 2018. 1\n[26] Matthew D Zeiler and Rob Fergus. Visualizing and under-\nstanding convolutional networks. In European conference on\ncomputer vision, pages 818–833. Springer, 2014. 1\n[27] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan\nBrandt, Xiaohui Shen, and Stan Sclaroff. Top-down neu-\nral attention by excitation backprop. International Journal\nof Computer Vision, 126(10):1084–1102, 2018. 1\n[28] Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical\ngeometry of deep neural networks. In International Confer-\nence on Machine Learning, pages 5824–5832. PMLR, 2018.\n2\n[29] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,\nand Antonio Torralba. Learning deep features for discrimina-\ntive localization. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2921–2929,\n2016. 1\n\n\n---\n\n"

def output_info(content, start_time, model_info):
    end_time = time.time()
    time_taken = end_time - start_time
    time_taken = round(time_taken * 10) / 10

    output = f"For {model_info}, time taken = " + str(time_taken)
    logging.info(output)
    logging.info(f"  --> {content}")
    st.write(output)

import cohere
import streamlit as st
cohere_key = st.secrets['cohere_key']

def do_cohere():
    cohere_key = st.secrets['cohere_key']
    co = cohere.Client(cohere_key)
    #response = co.tokenize(
     #   text = system_message, model='command')
    events = co.chat_stream(
                    model='command-r',
                    message=question_to_ask,
                    temperature=0,       
                    max_tokens=1500,
                    prompt_truncation='AUTO',
                    connectors=[],
                    documents=[]
    )

    response_text=""
    for event in events:
        if event.event_type=="text-generation":
            response_text = response_text + str(event.text)
    return response_text

model="Cohere"
start_time = time.time()
content = do_cohere()
st.write(content)
output_info(content, start_time, model_info=model)
st.write("done calcs...")
    
